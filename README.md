
# Internal Website Intelligence & Contact Discovery Tool

A full-stack internal web application that allows authenticated users to submit a company website URL and automatically extract structured company and contact information using web scraping and AI.

The system scrapes public pages, extracts contact details, formats results into validated JSON using an LLM, stores them in a database, and displays them in a clean UI with history tracking.

ğŸ“¦ DELIVERABLE 1: Source Code Structure

internal-website-intel/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ database.py
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ deps.py
â”‚   â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚       â”œâ”€â”€ auth.py
â”‚   â”‚   â”‚       â””â”€â”€ scans.py
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ security.py
â”‚   â”‚   â”‚   â””â”€â”€ users.py
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ scan.py
â”‚   â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ auth.py
â”‚   â”‚   â”‚   â””â”€â”€ scan.py
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ultimate_scraper.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ultimate_extractor.py
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_service.py
â”‚   â”‚   â”‚   â””â”€â”€ database_service.py
â”‚   â”‚   â”œâ”€â”€ middleware/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ rate_limit.py
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â””â”€â”€ validators.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ .env.example
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ layout.tsx
â”‚   â”‚   â”œâ”€â”€ page.tsx
â”‚   â”‚   â”œâ”€â”€ globals.css
â”‚   â”‚   â”œâ”€â”€ login/
â”‚   â”‚   â”‚   â””â”€â”€ page.tsx
â”‚   â”‚   â”œâ”€â”€ dashboard/
â”‚   â”‚   â”‚   â””â”€â”€ page.tsx
â”‚   â”‚   â””â”€â”€ history/
â”‚   â”‚       â”œâ”€â”€ page.tsx
â”‚   â”‚       â””â”€â”€ [id]/
â”‚   â”‚           â””â”€â”€ page.tsx
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ Header.tsx
â”‚   â”‚   â”œâ”€â”€ ScanForm.tsx
â”‚   â”‚   â”œâ”€â”€ ScanResult.tsx
â”‚   â”‚   â””â”€â”€ HistoryList.tsx
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ api.ts
â”‚   â”‚   â””â”€â”€ auth.ts
â”‚   â”œâ”€â”€ types/
â”‚   â”‚   â””â”€â”€ index.ts
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ package-lock.json
â”‚   â”œâ”€â”€ tsconfig.json
â”‚   â”œâ”€â”€ next.config.js
â”‚   â”œâ”€â”€ tailwind.config.ts
â”‚   â”œâ”€â”€ postcss.config.js
â”‚   â”œâ”€â”€ .env.local.example
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md (main)

âœ¨ Features
Core

Login-only authentication using JWT (no signup)

Website scraping (homepage + relevant pages like About/Contact)

Contact extraction (emails, phone numbers, social links, addresses)

AI-powered structuring using Google Gemini (validated JSON output)

Database persistence using SQLite + SQLAlchemy

History page with pagination and delete support

Protected frontend routes

Bonus

Rate limiting (login & scan endpoints)

Multi-strategy scraping (BeautifulSoup + Selenium)

JSON export for scan results

Progress indicators & loading states

Responsive UI with Tailwind CSS

ğŸ›  Tech Stack

Frontend: Next.js (App Router), TypeScript, Tailwind CSS

Backend: FastAPI (Python)

Database: SQLite (via SQLAlchemy ORM)

AI: Google Gemini (structured JSON output)

Scraping: BeautifulSoup + Selenium

Auth: JWT (python-jose)

Rate Limiting: SlowAPI

ğŸš€ Setup & Run
Backend
cd backend
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
cp .env.example .env
uvicorn app.main:app --reload


Backend runs at: http://localhost:8000
API Docs: http://localhost:8000/docs

Frontend
cd frontend
npm install
cp .env.local.example .env.local
npm run dev


Frontend runs at: http://localhost:3000

ğŸ” Default Credentials

admin / password123

demo / password123

ğŸ§  How It Works (High-Level)

User logs in â†’ backend issues JWT

User submits a website URL

Backend scrapes relevant pages

Contact info is extracted deterministically

AI formats data into structured JSON

Output is validated with Pydantic

Result is saved to database

Frontend displays result and history

âš™ï¸ Design Notes

SQLite chosen for easy local development (swap to PostgreSQL via env)

AI output is strictly validated before persistence

Scraping is best-effort due to website variability

JWT stored in localStorage for simplicity (cookies recommended for prod)

ğŸ“Œ Known Limitations

History detail route (/history/[id]) is optional polish; results are already viewable via History page

Scraping may fail on heavily protected websites

ğŸ“„ License

Internal demo project for assignment evaluation.

